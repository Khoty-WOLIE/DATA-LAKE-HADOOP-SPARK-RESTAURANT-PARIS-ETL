# DATA-LAKE-HADOOP-SPARK-RESTAURANT-PARIS-ETL
Projet dâ€™architecture Data Lake local avec traitement distribuÃ© et analyse des donnÃ©es OSM via WSL sous Windows, avec HDFS, Hive, Spark SQL, et visualisation des donnÃ©es dans Jupyter Notebook.

---

## ğŸš€ Objectif

Mettre en Å“uvre un **Data Lake fonctionnel** avec la stack suivante :
- **Hadoop HDFS** : stockage distribuÃ© local
- **Hive** : traitement SQL (optionnel)
- **Spark SQL** : traitement analytique performant
- **PySpark + Pandas** : analyse programmÃ©e en Python
- **Jupyter Notebook** : visualisation de donnÃ©es
- **Bronze / Silver / Gold layers** : architecture analytique

---

## ğŸ“ Architecture en locale

```bash
ğŸ“‚ hadoop_datalake
â”œâ”€â”€ hadoop-3.4.1/         # Hadoop HDFS
â”œâ”€â”€ hive/                 # Hive 3.1.3 (optionnel)
â”œâ”€â”€ spark/                # Spark 3.5.0
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Analyse_Restaurants_Paris.ipynb  # Notebook principal âœ…
â”œâ”€â”€ restaurants_gold.csv  # DonnÃ©es exportÃ©es pour analyse
```

---

## ğŸ§± Ã‰tapes rÃ©alisÃ©es

### ğŸ”§ Setup Environnement

- [x] WSL (Ubuntu 22.04) + Java 8
- [x] Installation de Hadoop 3.4.1
- [x] Configuration HDFS + formatage `namenode`
- [x] Lancement manuel des dÃ©mons `namenode` et `datanode`
- [x] Interface Web HDFS sur `http://localhost:9870`

---

### ğŸ—ƒï¸ Structure Data Lake

- [x] ğŸ“¦ **Bronze** : fichiers `.osm` OpenStreetMap
- [x] ğŸ§¼ **Silver** : extraction des restaurants â†’ `.csv`
- [x] ğŸ“Š **Gold** : agrÃ©gation, analyse, transformation

---

### ğŸ˜ Hive (facultatif)

- [x] Hive 3.1.3 installÃ©
- [x] Table externe sur fichier HDFS
- [x] RequÃªtes SQL simples (`SELECT`, `LIMIT`)
- [ ] âŒ MapReduce local instable sur WSL (utilisÃ© Spark SQL Ã  la place)

---

### âš¡ Spark SQL

- [x] Spark 3.5.0 installÃ©
- [x] Lecture du CSV depuis HDFS
- [x] RequÃªtes SQL complexes (`GROUP BY`, `ORDER BY`)
- [x] Export en Parquet
- [x] Utilisation de PySpark + Spark SQL

---

### ğŸ““ Jupyter Notebook

- [x] Lancement via Anaconda
- [x] Lecture du fichier `restaurants_gold.csv`
- [x] Analyse exploratoire avec `pandas`
- [x] Visualisation avec `seaborn`, `matplotlib`
- [x] Statistiques descriptives
- [x] Graphique des restaurants les plus frÃ©quents

---

## ğŸ”œ Autre Ã©tapes

### ğŸ“Š Connexions BI / visualisation

- [ ] **Apache Superset**
  - Connexion via Hive ou Parquet
  - Table interactive et dashboards
- [ ] **Power BI**
  - Lecture CSV/Parquet depuis HDFS (via `hadoop fs -copyToLocal`)
  - Ou connexion directe Hive via ODBC
- [ ] **Extension Notebook**
  - Visualisation sur carte avec `folium` ou `geopandas`
  - Clustering avec `scikit-learn`

---

## ğŸ§  Technologies utilisÃ©es

| Outil         | Version       | RÃ´le                               |
|---------------|---------------|------------------------------------|
| Hadoop        | 3.4.1         | Stockage HDFS                      |
| Hive          | 3.1.3         | SQL sur fichiers HDFS (optionnel)  |
| Spark         | 3.5.0         | Traitement distribuÃ© et SQL        |
| PySpark       | incl. Spark   | RequÃªtes et transformation Python  |
| Python        | 3.10+         | Parsing, visualisation, notebooks  |
| Jupyter       | via Anaconda  | Notebook interactif                |
| Java          | 8 (OpenJDK)   | Support Hadoop / Hive / Spark      |

---

## ğŸ“¦ Librairies Python utilisÃ©es

```bash
pandas
seaborn
matplotlib
pyspark
findspark
```

---

## ğŸ“„ Notebook principal

ğŸ”— Fichier : [`notebooks/Analyse_Restaurants_Paris.ipynb`](https://github.com/Khoty-WOLIE/DATA-LAKE-HADOOP-SPARK-RESTAURANT-PARIS-ETL/blob/main/Analyse%20des%20restaurants%20Parisiens.ipynb)

Contenu :
- Chargement des donnÃ©es depuis CSV
- Statistiques descriptives
- Distribution des restaurants
- Top 10 des noms les plus frÃ©quents
- Analyse textuelle des noms

---

## ğŸ› ï¸ Commandes utiles

```bash
# Lancer Hadoop
hdfs --daemon start namenode
hdfs --daemon start datanode

# Lire un fichier depuis HDFS
hdfs dfs -cat /datalake/gold/paris/restaurants_gold.csv

# Lancer Spark SQL
spark-sql

# Lancer PySpark
pyspark
```

---

## ğŸ“Œ Notes

- âš ï¸ Hive sur WSL ne supporte pas bien MapReduce â†’ Spark SQL prÃ©fÃ©rÃ©
- âœ… Spark SQL fonctionne parfaitement pour les requÃªtes complexes
- ğŸ“ˆ Jupyter permet une visualisation rapide et fluide
- ğŸ” Tous les fichiers sont stockÃ©s dans `/datalake` sur HDFS

---

## ğŸ“· Captures possibles

- Interface HDFS (http://localhost:9870)
- Web UI de Spark (`http://localhost:4040`)
- Graphiques matplotlib / seaborn du notebook

---

## ğŸ¤ CrÃ©dits & Inspiration

- [OpenStreetMap](https://www.openstreetmap.org/)
- [Apache Hadoop](https://hadoop.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Hive](https://hive.apache.org/)
